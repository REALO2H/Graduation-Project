{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      image  level  inverted\n",
      "0   10_left      0     False\n",
      "1  10_right      0     False\n",
      "2   13_left      0     False\n",
      "3  13_right      0     False\n",
      "4   15_left      1     False\n",
      "Saved cropped image: ../CroppedImages/10_left.jpeg\n",
      "Saved cropped image: ../CroppedImages/10_right.jpeg\n",
      "Saved cropped image: ../CroppedImages/13_left.jpeg\n",
      "Saved cropped image: ../CroppedImages/13_right.jpeg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "labels_df = pd.read_csv('../labels_cleaned.csv') \n",
    "labels_df['inverted'] = False\n",
    "print(labels_df.head())\n",
    "\n",
    "image_dir = '../Images/train/'\n",
    "output_dir = '../CroppedImages/'\n",
    "\n",
    "def load_image(image_name):\n",
    "    file_extension = '.jpeg'\n",
    "    filename = f\"{image_name}{file_extension}\"\n",
    "    file_path = os.path.join(image_dir, filename)\n",
    "    return Image.open(file_path)\n",
    "\n",
    "def crop_to_circular_retina(pil_img, use_otsu=True):\n",
    "    \"\"\"\n",
    "    Removes the black background and crops the image to a circular retina region.\n",
    "\n",
    "    :param pil_img: Input PIL image\n",
    "    :param use_otsu: Whether to use Otsu's thresholding for automatic threshold selection\n",
    "    :return: Cropped circular retina PIL image\n",
    "    \"\"\"\n",
    "    # 1. Convert from PIL to OpenCV format (BGR)\n",
    "    cv_img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # 2. Convert to grayscale\n",
    "    gray = cv2.cvtColor(cv_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 3. Threshold to isolate the retina region\n",
    "    if use_otsu:\n",
    "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    else:\n",
    "        _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 4. Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        print(\"No contours found; possibly no distinct circular region.\")\n",
    "        return pil_img  # Return the original image if no contour is found\n",
    "\n",
    "    # 5. Get the largest contour (the retina region)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # 6. Find the enclosing circle for the retina\n",
    "    (x, y), radius = cv2.minEnclosingCircle(largest_contour)\n",
    "    center = (int(x), int(y))\n",
    "    radius = int(radius)\n",
    "\n",
    "    # 7. Create a circular mask\n",
    "    mask = np.zeros_like(gray)\n",
    "    cv2.circle(mask, center, radius, 255, -1)\n",
    "\n",
    "    # Apply the mask to keep only the circular retina\n",
    "    circular_cropped = cv2.bitwise_and(cv_img, cv_img, mask=mask)\n",
    "\n",
    "    # 8. Crop the bounding box of the circle to reduce image size\n",
    "    x_start = max(center[0] - radius, 0)\n",
    "    y_start = max(center[1] - radius, 0)\n",
    "    x_end = min(center[0] + radius, cv_img.shape[1])\n",
    "    y_end = min(center[1] + radius, cv_img.shape[0])\n",
    "    cropped_img = circular_cropped[y_start:y_end, x_start:x_end]\n",
    "\n",
    "    # 9. Convert back to PIL format\n",
    "    cropped_pil = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
    "    return cropped_pil\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process all images in the dataset\n",
    "for i in range(1):\n",
    "    image_name = labels_df.iloc[i]['image']\n",
    "    sample_image = load_image(image_name)\n",
    "\n",
    "    # Crop the retina region with circular cropping\n",
    "    cropped_retina = crop_to_circular_retina(sample_image, use_otsu=True)\n",
    "\n",
    "    # Show the cropped image\n",
    "    cropped_retina.show()\n",
    "\n",
    "    # Save the cropped image\n",
    "    output_path = os.path.join(output_dir, f\"{image_name}.jpeg\")\n",
    "    cropped_retina.save(output_path)\n",
    "    print(f\"Saved cropped image: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inversion checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_inverted(image):\n",
    "    try:\n",
    "        # Ensure the image is fully loaded\n",
    "        image.load()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image data: {e}\")\n",
    "        raise e  # Re-raise the exception\n",
    "\n",
    "    try:\n",
    "        # Ensure image is in RGB mode\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        # Convert PIL image to OpenCV format\n",
    "        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting image to OpenCV format: {e}\")\n",
    "        raise e  # Re-raise the exception\n",
    "\n",
    "    try:\n",
    "        # Detect features\n",
    "        optic_nerve_pos = detect_optic_nerve(cv_image)\n",
    "        macula_pos = detect_macula(cv_image)\n",
    "        notch_present = detect_notch(cv_image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature detection: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Ensure features were detected\n",
    "    if optic_nerve_pos is None or macula_pos is None:\n",
    "        print(\"Could not detect optic nerve or macula.\")\n",
    "        return False  # Or handle as per your needs\n",
    "\n",
    "    # Calculate optic nerve midline (y-coordinate)\n",
    "    optic_nerve_midline_y = optic_nerve_pos['y']\n",
    "\n",
    "    # Compare macula position to optic nerve midline\n",
    "    macula_higher = macula_pos['y'] < optic_nerve_midline_y\n",
    "\n",
    "    # Determine inversion based on criteria\n",
    "    if (macula_higher) or (not notch_present):\n",
    "        # Image is inverted\n",
    "        return True\n",
    "    else:\n",
    "        # Image is not inverted\n",
    "        return False\n",
    "\n",
    "def correct_inversion(image):\n",
    "    # Flip the image vertically\n",
    "    corrected_image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    return corrected_image\n",
    "\n",
    "\n",
    "    #<<functions for feature detection>>\n",
    "\n",
    "    # Optic never detection\n",
    "def detect_optic_nerve(image):\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "\n",
    "    # Apply Gaussian Blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "    # Apply thresholding to segment bright areas\n",
    "    _, thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Assume the largest bright area is the optic nerve\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        # Calculate the center of the optic nerve\n",
    "        M = cv2.moments(largest_contour)\n",
    "        if M['m00'] != 0:\n",
    "            cx = int(M['m10'] / M['m00'])  # x-coordinate\n",
    "            cy = int(M['m01'] / M['m00'])  # y-coordinate\n",
    "            return {'x': cx, 'y': cy}\n",
    "    return None\n",
    "\n",
    "\n",
    "#Macula detection\n",
    "\n",
    "def detect_macula(image):\n",
    "    # Convert to grayscale if necessary\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # green_channel = image[:, :, 1]\n",
    "    else:\n",
    "        gray_image = image\n",
    "        # green_channel = image\n",
    "        \n",
    "\n",
    "    # Image dimensions\n",
    "\n",
    "    height, width = gray_image.shape\n",
    "    # height, width = green_channel.shape\n",
    "\n",
    "    # Define the central region (e.g., central 50% of the image)\n",
    "    x_start = int(width * 0.2)\n",
    "    x_end = int(width * 0.8)\n",
    "    y_start = int(height * 0.2)\n",
    "    y_end = int(height * 0.8)\n",
    "\n",
    "    # Crop the central region\n",
    "    central_region = gray_image[y_start:y_end, x_start:x_end]\n",
    "    # central_region = green_channel[y_start:y_end, x_start:x_end]\n",
    "\n",
    "    #Gaussian Blur \n",
    "    blurred = cv2.GaussianBlur( central_region, (5, 5), 0,cv2.BORDER_DEFAULT)\n",
    "\n",
    "    #Thresholding\n",
    "    res,thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    #Contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        print(\"No dark regions detected in the central area.\")\n",
    "        return {'x': 0, 'y': 0}\n",
    "    \n",
    "    # Assume the largest dark contour corresponds to the macula\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Calculate the centroid of the largest contour\n",
    "    M = cv2.moments(largest_contour)\n",
    "    if M['m00'] == 0:\n",
    "        print(\"Zero division error while calculating centroid.\")\n",
    "        return {'x': 0, 'y': 0}\n",
    "\n",
    "    cX = int(M['m10'] / M['m00']) + x_start\n",
    "    cY = int(M['m01'] / M['m00']) + y_start\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create a resizable window\n",
    "    cv2.namedWindow('Grayscale Image', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Grayscale Image', 800, 600)  # Set to desired dimensions\n",
    "    cv2.namedWindow('Blurred Image', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Blurred Image', 800, 600)  \n",
    "    cv2.namedWindow('Thresholded Central Region', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Thresholded Central Region', 800, 600)  \n",
    "    cv2.namedWindow('Detected Macula', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Detected Macula', 800, 600)  \n",
    "    cv2.namedWindow('Central Region', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Central Region', 800, 600)\n",
    "    cv2.namedWindow('res', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Central Region', 800, 600) \n",
    "    cv2.circle(central_region, (cX, cY), 30, (0, 0, 255), -1)  # Red circle for macula\n",
    "    cv2.circle(image, (cX, cY), 30, (0, 0, 255), -1)  # Red circle for macula\n",
    "    cv2.circle(thresh, (cX, cY), 30, (0, 0, 255), -1)  # Red circle for macula\n",
    "   \n",
    "\n",
    "    cv2.imshow('Grayscale Image', gray_image)\n",
    "    # cv2.imshow('Grayscale Image',green_channel)\n",
    "    cv2.imshow('Blurred Image', blurred)\n",
    "    cv2.imshow('Thresholded Central Region', thresh)\n",
    "    cv2.imshow('res', res)\n",
    "    cv2.imshow('Detected Macula', image)\n",
    "    cv2.imshow('Central Region', central_region)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    return {'x': cX, 'y': cY}\n",
    "\n",
    "#Notch detection\n",
    "def detect_notch(image):\n",
    "    # Convert to grayscale if necessary\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "\n",
    "    # Crop the edges of the image where the notch is expected\n",
    "    height, width = gray_image.shape\n",
    "    edge_width = int(width * 0.05)  # Adjust as needed\n",
    "\n",
    "    # Left edge\n",
    "    left_edge = gray_image[:, :edge_width]\n",
    "    # Right edge\n",
    "    right_edge = gray_image[:, -edge_width:]\n",
    "\n",
    "    # Combine edges\n",
    "    edges = [left_edge, right_edge]\n",
    "\n",
    "    # Look for contours in the edge regions\n",
    "    for edge in edges:\n",
    "        # Apply thresholding\n",
    "        _, thresh = cv2.threshold(edge, 35, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Analyze contours\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area > 100:  # Adjust threshold based on expected notch size\n",
    "                # Approximate contour to a polygon\n",
    "                approx = cv2.approxPolyDP(cnt, 0.04 * cv2.arcLength(cnt, True), True)\n",
    "                # Check for square, triangle, or circle\n",
    "                if len(approx) == 3 or len(approx) == 4 or len(approx) > 8:\n",
    "                    return True  # Notch detected\n",
    "    return False  # No notch detected \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 10_left\n",
      "Image 10_left inverted: True\n",
      "Processing image 10_right\n",
      "Could not detect optic nerve or macula.\n",
      "Image 10_right inverted: False\n",
      "Processing image 13_left\n",
      "Image 13_left inverted: True\n",
      "Processing image 13_right\n",
      "Image 13_right inverted: True\n",
      "Processing image 15_left\n",
      "Could not detect optic nerve or macula.\n",
      "Image 15_left inverted: False\n",
      "Processing image 15_right\n",
      "Image 15_right inverted: True\n",
      "Processing image 16_left\n",
      "Image 16_left inverted: True\n",
      "Processing image 16_right\n",
      "Image 16_right inverted: True\n",
      "Processing image 17_left\n",
      "Image 17_left inverted: True\n",
      "Processing image 17_right\n",
      "Image 17_right inverted: True\n",
      "Processing image 19_left\n",
      "Image 19_left inverted: True\n",
      "Processing image 19_right\n",
      "Image 19_right inverted: True\n",
      "Processing image 20_left\n",
      "Image 20_left inverted: True\n",
      "Processing image 20_right\n",
      "Image 20_right inverted: True\n",
      "Processing image 21_left\n",
      "Image 21_left inverted: True\n",
      "Processing image 21_right\n",
      "Image 21_right inverted: True\n",
      "Processing image 22_left\n",
      "Image 22_left inverted: True\n",
      "Processing image 22_right\n",
      "Could not detect optic nerve or macula.\n",
      "Image 22_right inverted: False\n",
      "Processing image 23_left\n",
      "Could not detect optic nerve or macula.\n",
      "Image 23_left inverted: False\n",
      "Processing image 23_right\n",
      "Could not detect optic nerve or macula.\n",
      "Image 23_right inverted: False\n",
      "Processing image 25_left\n",
      "Could not detect optic nerve or macula.\n",
      "Image 25_left inverted: False\n",
      "Processing image 25_right\n",
      "Image 25_right inverted: True\n",
      "Processing image 30_left\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m         inverted \u001b[38;5;241m=\u001b[39m \u001b[43mis_inverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m         labels_df\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minverted\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m inverted\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inverted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minverted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m, in \u001b[0;36mis_inverted\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Detect features\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     optic_nerve_pos \u001b[38;5;241m=\u001b[39m detect_optic_nerve(cv_image)\n\u001b[1;32m---> 22\u001b[0m     macula_pos \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_macula\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     notch_present \u001b[38;5;241m=\u001b[39m detect_notch(cv_image)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[5], line 166\u001b[0m, in \u001b[0;36mdetect_macula\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    164\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDetected Macula\u001b[39m\u001b[38;5;124m'\u001b[39m, image)\n\u001b[0;32m    165\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCentral Region\u001b[39m\u001b[38;5;124m'\u001b[39m, central_region)\n\u001b[1;32m--> 166\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: cX, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m: cY}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for index, row in labels_df.iterrows():\n",
    "    image_name = row['image']\n",
    "    print(f\"Processing image {image_name}\")\n",
    "    image = load_image(image_name)\n",
    "\n",
    "    if image is not None:\n",
    "        try:\n",
    "            inverted = is_inverted(image)\n",
    "            labels_df.at[index, 'inverted'] = inverted\n",
    "            print(f\"Image {image_name} inverted: {inverted}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_name}: {e}\")\n",
    "            labels_df.at[index, 'inverted'] = None\n",
    "    else:\n",
    "        print(f\"Skipping image {image_name} due to loading error.\")\n",
    "        labels_df.at[index, 'inverted'] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test on a sample image\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sample_image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m(labels_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m8\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m cv_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(np\u001b[38;5;241m.\u001b[39marray(sample_image), cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Detect features\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_image' is not defined"
     ]
    }
   ],
   "source": [
    "# Test on a sample image\n",
    "sample_image = load_image(labels_df.iloc[8]['image'])\n",
    "cv_image = cv2.cvtColor(np.array(sample_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Detect features\n",
    "optic_nerve_pos = detect_optic_nerve(cv_image)\n",
    "macula_pos = detect_macula(cv_image)\n",
    "notch_present = detect_notch(cv_image)\n",
    "\n",
    "print(f\"Optic Nerve Position: {optic_nerve_pos}\")\n",
    "print(f\"Macula Position: {macula_pos}\")\n",
    "print(f\"Notch Present: {notch_present}\")\n",
    "\n",
    "# Display the image with detected points\n",
    "if optic_nerve_pos and macula_pos:\n",
    "    cv2.circle(cv_image, (optic_nerve_pos['x'], optic_nerve_pos['y']), 50, (0, 255, 0), -1)\n",
    "    cv2.circle(cv_image, (macula_pos['x'], macula_pos['y']), 50, (0, 0, 255), -1)\n",
    "\n",
    "     # Create a resizable window\n",
    "    cv2.namedWindow('Detected Features', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Detected Features', 800, 600)  # Set to desired dimensions\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    cv2.imshow('Detected Features', cv_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
