{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      image  level  inverted\n",
      "0   10_left      0     False\n",
      "1  10_right      0     False\n",
      "2   13_left      0     False\n",
      "3  13_right      0     False\n",
      "4   15_left      1     False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "#s\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv('labels_cleaned.csv') \n",
    "labels_df['inverted'] = False\n",
    "print(labels_df.head())\n",
    "\n",
    "image_dir = '../Images\\\\train\\\\' \n",
    "def load_image(image_name):\n",
    "    file_extension = '.jpeg'\n",
    "    filename = f\"{image_name}{file_extension}\"\n",
    "    file_path = os.path.join(image_dir, filename)\n",
    "    return Image.open(file_path)\n",
    "\n",
    "for i in range (1):\n",
    "    sample_image = load_image(labels_df.iloc[i]['image'])\n",
    "    sample_image.show()\n",
    "\n",
    "\n",
    "# Cleaning the dataset\n",
    "\n",
    "# valid_rows = []\n",
    "# for index, row in labels_df.iterrows():\n",
    "#     image_name = row['image']\n",
    "#     image_path = os.path.join(image_dir, f\"{image_name}.jpeg\")\n",
    "#     if os.path.exists(image_path):\n",
    "#         valid_rows.append(row)\n",
    "\n",
    "# cleaned_df = pd.DataFrame(valid_rows)\n",
    "# cleaned_df.to_csv('labels_cleaned.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device: cuda\n",
      "Torch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 2050\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")  # Should print \"Using device: cuda\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'\u001b[31m\u001b[1mbatch_size\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['batch=16'].\n\n    Arguments received: ['yolo', '--f=\"c:\\\\Users\\\\Omar Halabi\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3cc9cf64189d31720f5c15f9845a967643ad4c18d.json\"']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of frozenset({'obb', 'classify', 'pose', 'segment', 'detect'})\n                MODE (required) is one of frozenset({'track', 'benchmark', 'predict', 'train', 'val', 'export'})\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['heatmap', 'queue', 'speed', 'workout', 'analytics', 'trackzone', 'inference'] source=\"path/to/video/file.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[16], line 10\u001b[0m\n    model.train(data=\"D:\\Graduation project\\Graduation-Project\\My-First-Project-4\\data.yaml\", epochs=50, imgsz=640, batch_size =32, device=device, workers=0, mosaic=0, plots=True)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Omar Halabi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\model.py:803\u001b[0m in \u001b[0;35mtrain\u001b[0m\n    self.trainer = (trainer or self._smart_load(\"trainer\"))(overrides=args, _callbacks=self.callbacks)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Omar Halabi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\trainer.py:102\u001b[0m in \u001b[0;35m__init__\u001b[0m\n    self.args = get_cfg(cfg, overrides)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\Omar Halabi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\cfg\\__init__.py:306\u001b[0m in \u001b[0;35mget_cfg\u001b[0m\n    check_dict_alignment(cfg, overrides)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\Omar Halabi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\cfg\\__init__.py:493\u001b[1;36m in \u001b[1;35mcheck_dict_alignment\u001b[1;36m\n\u001b[1;33m    raise SyntaxError(string + CLI_HELP_MSG) from e\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m '\u001b[31m\u001b[1mbatch_size\u001b[0m' is not a valid YOLO argument. Similar arguments are i.e. ['batch=16'].\n\n    Arguments received: ['yolo', '--f=\"c:\\\\Users\\\\Omar Halabi\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3cc9cf64189d31720f5c15f9845a967643ad4c18d.json\"']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of frozenset({'obb', 'classify', 'pose', 'segment', 'detect'})\n                MODE (required) is one of frozenset({'track', 'benchmark', 'predict', 'train', 'val', 'export'})\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['heatmap', 'queue', 'speed', 'workout', 'analytics', 'trackzone', 'inference'] source=\"path/to/video/file.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf = Roboflow(api_key=\"YkB7AdZqhiQmoKyu0FvN\")\n",
    "project = rf.workspace(\"graduation-project-f8ud5\").project(\"my-first-project-nxzes\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"yolov8\")\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Start training on your custom dataset\n",
    "model.train(data=\"D:\\Graduation project\\Graduation-Project\\My-First-Project-4\\data.yaml\", epochs=50, imgsz=640, batch =32, device=device, workers=0, mosaic=0, plots=True)\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion checking algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_inverted(image):\n",
    "    try:\n",
    "        # Ensure the image is fully loaded\n",
    "        image.load()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image data: {e}\")\n",
    "        raise e  # Re-raise the exception\n",
    "\n",
    "    try:\n",
    "        # Ensure image is in RGB mode\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        # Convert PIL image to OpenCV format\n",
    "        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting image to OpenCV format: {e}\")\n",
    "        raise e  # Re-raise the exception\n",
    "\n",
    "    try:\n",
    "        # Detect features\n",
    "        optic_nerve_pos = detect_optic_nerve(cv_image)\n",
    "        macula_pos = detect_macula(cv_image)\n",
    "        notch_present = detect_notch(cv_image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature detection: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Ensure features were detected\n",
    "    if optic_nerve_pos is None or macula_pos is None:\n",
    "        print(\"Could not detect optic nerve or macula.\")\n",
    "        return False  # Or handle as per your needs\n",
    "\n",
    "    # Calculate optic nerve midline (y-coordinate)\n",
    "    optic_nerve_midline_y = optic_nerve_pos['y']\n",
    "\n",
    "    # Compare macula position to optic nerve midline\n",
    "    macula_higher = macula_pos['y'] < optic_nerve_midline_y\n",
    "\n",
    "    # Determine inversion based on criteria\n",
    "    if (macula_higher) or (not notch_present):\n",
    "        # Image is inverted\n",
    "        return True\n",
    "    else:\n",
    "        # Image is not inverted\n",
    "        return False\n",
    "\n",
    "def correct_inversion(image):\n",
    "    # Flip the image vertically\n",
    "    corrected_image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    return corrected_image\n",
    "\n",
    "\n",
    "    #<<functions for feature detection>>\n",
    "\n",
    "    # Optic never detection\n",
    "def detect_optic_nerve(image):\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "\n",
    "    # Apply Gaussian Blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "    # Apply thresholding to segment bright areas\n",
    "    _, thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Assume the largest bright area is the optic nerve\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        # Calculate the center of the optic nerve\n",
    "        M = cv2.moments(largest_contour)\n",
    "        if M['m00'] != 0:\n",
    "            cx = int(M['m10'] / M['m00'])  # x-coordinate\n",
    "            cy = int(M['m01'] / M['m00'])  # y-coordinate\n",
    "            return {'x': cx, 'y': cy}\n",
    "    return None\n",
    "\n",
    "\n",
    "#Macula detection\n",
    "\n",
    "def detect_macula(image):\n",
    "    # Convert to grayscale if necessary\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # green_channel = image[:, :, 1]\n",
    "    else:\n",
    "        gray_image = image\n",
    "        # green_channel = image\n",
    "        \n",
    "\n",
    "    # Image dimensions\n",
    "\n",
    "    height, width = gray_image.shape\n",
    "    # height, width = green_channel.shape\n",
    "\n",
    "    # Define the central region (e.g., central 50% of the image)\n",
    "    x_start = int(width * 0.2)\n",
    "    x_end = int(width * 0.8)\n",
    "    y_start = int(height * 0.2)\n",
    "    y_end = int(height * 0.8)\n",
    "\n",
    "    # Crop the central region\n",
    "    central_region = gray_image[y_start:y_end, x_start:x_end]\n",
    "    # central_region = green_channel[y_start:y_end, x_start:x_end]\n",
    "\n",
    "    #Gaussian Blur \n",
    "    blurred = cv2.GaussianBlur( central_region, (5, 5), 0,cv2.BORDER_DEFAULT)\n",
    "\n",
    "    #Thresholding\n",
    "    res,thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    #Contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        print(\"No dark regions detected in the central area.\")\n",
    "        return {'x': 0, 'y': 0}\n",
    "    \n",
    "    # Assume the largest dark contour corresponds to the macula\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Calculate the centroid of the largest contour\n",
    "    M = cv2.moments(largest_contour)\n",
    "    if M['m00'] == 0:\n",
    "        print(\"Zero division error while calculating centroid.\")\n",
    "        return {'x': 0, 'y': 0}\n",
    "\n",
    "    cX = int(M['m10'] / M['m00']) + x_start\n",
    "    cY = int(M['m01'] / M['m00']) + y_start\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create a resizable window\n",
    "    cv2.namedWindow('Grayscale Image', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Grayscale Image', 800, 600)  # Set to desired dimensions\n",
    "    cv2.namedWindow('Blurred Image', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Blurred Image', 800, 600)  \n",
    "    cv2.namedWindow('Thresholded Central Region', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Thresholded Central Region', 800, 600)  \n",
    "    cv2.namedWindow('Detected Macula', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Detected Macula', 800, 600)  \n",
    "    cv2.namedWindow('Central Region', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Central Region', 800, 600)\n",
    "    cv2.namedWindow('res', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Central Region', 800, 600) \n",
    "    cv2.circle(central_region, (cX, cY), 30, (0, 0, 255), -1)  # Red circle for macula\n",
    "    cv2.circle(image, (cX, cY), 30, (0, 0, 255), -1)  # Red circle for macula\n",
    "    cv2.circle(thresh, (cX, cY), 30, (0, 0, 255), -1)  # Red circle for macula\n",
    "   \n",
    "\n",
    "    cv2.imshow('Grayscale Image', gray_image)\n",
    "    # cv2.imshow('Grayscale Image',green_channel)\n",
    "    cv2.imshow('Blurred Image', blurred)\n",
    "    cv2.imshow('Thresholded Central Region', thresh)\n",
    "    cv2.imshow('res', res)\n",
    "    cv2.imshow('Detected Macula', image)\n",
    "    cv2.imshow('Central Region', central_region)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    return {'x': cX, 'y': cY}\n",
    "\n",
    "#Notch detection\n",
    "def detect_notch(image):\n",
    "    # Convert to grayscale if necessary\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "\n",
    "    # Crop the edges of the image where the notch is expected\n",
    "    height, width = gray_image.shape\n",
    "    edge_width = int(width * 0.05)  # Adjust as needed\n",
    "\n",
    "    # Left edge\n",
    "    left_edge = gray_image[:, :edge_width]\n",
    "    # Right edge\n",
    "    right_edge = gray_image[:, -edge_width:]\n",
    "\n",
    "    # Combine edges\n",
    "    edges = [left_edge, right_edge]\n",
    "\n",
    "    # Look for contours in the edge regions\n",
    "    for edge in edges:\n",
    "        # Apply thresholding\n",
    "        _, thresh = cv2.threshold(edge, 35, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Analyze contours\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area > 100:  # Adjust threshold based on expected notch size\n",
    "                # Approximate contour to a polygon\n",
    "                approx = cv2.approxPolyDP(cnt, 0.04 * cv2.arcLength(cnt, True), True)\n",
    "                # Check for square, triangle, or circle\n",
    "                if len(approx) == 3 or len(approx) == 4 or len(approx) > 8:\n",
    "                    return True  # Notch detected\n",
    "    return False  # No notch detected \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion cheking algorithm 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_inverted(image):\n",
    "    try:\n",
    "        # Ensure the image is fully loaded\n",
    "        image.load()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image data: {e}\")\n",
    "        raise e  # Re-raise the exception\n",
    "\n",
    "    try:\n",
    "        # Ensure image is in RGB mode\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        # Convert PIL image to OpenCV format\n",
    "        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting image to OpenCV format: {e}\")\n",
    "        raise e  # Re-raise the exception\n",
    "\n",
    "    try:\n",
    "        # Detect features\n",
    "        optic_nerve_pos = detect_optic_nerve(cv_image)\n",
    "        macula_pos = detect_macula(cv_image)\n",
    "        notch_present = detect_notch(cv_image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature detection: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Ensure features were detected\n",
    "    if optic_nerve_pos is None or macula_pos is None:\n",
    "        print(\"Could not detect optic nerve or macula.\")\n",
    "        return False  # Or handle as per your needs\n",
    "\n",
    "    # Calculate optic nerve midline (y-coordinate)\n",
    "    optic_nerve_midline_y = optic_nerve_pos['y']\n",
    "\n",
    "    # Compare macula position to optic nerve midline\n",
    "    macula_higher = macula_pos['y'] < optic_nerve_midline_y\n",
    "\n",
    "    # Determine inversion based on criteria\n",
    "    if (macula_higher) or (not notch_present):\n",
    "        # Image is inverted\n",
    "        return True\n",
    "    else:\n",
    "        # Image is not inverted\n",
    "        return False\n",
    "\n",
    "def correct_inversion(image):\n",
    "    # Flip the image vertically\n",
    "    corrected_image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    return corrected_image\n",
    "\n",
    "# ---------------------------\n",
    "# **Optic Nerve Detection** (Brightest 30x30 patch)\n",
    "# ---------------------------\n",
    "def detect_optic_nerve(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Sliding window to find the brightest 30x30 region\n",
    "    max_brightness = 0\n",
    "    optic_nerve_center = (0, 0)\n",
    "\n",
    "    for y in range(0, gray_image.shape[0] - 30, 10):  \n",
    "        for x in range(0, gray_image.shape[1] - 30, 10):  \n",
    "            patch = gray_image[y:y+30, x:x+30]\n",
    "            mean_brightness = np.mean(patch)\n",
    "\n",
    "            if mean_brightness > max_brightness:\n",
    "                max_brightness = mean_brightness\n",
    "                optic_nerve_center = (x + 15, y + 15)  # Center of 30x30 patch\n",
    "\n",
    "    return {'x': optic_nerve_center[0], 'y': optic_nerve_center[1]}\n",
    "\n",
    "# ---------------------------\n",
    "# **Macula Detection** (Darkest 30x30 patch)\n",
    "# ---------------------------\n",
    "def detect_macula(image):\n",
    "    \"\"\"\n",
    "    Detects the macula by identifying the darkest region **inside the retina** while ignoring the black background.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert to grayscale if necessary\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Step 1: Create a mask to exclude the background\n",
    "    _, mask = cv2.threshold(gray_image, 10, 255, cv2.THRESH_BINARY)  # Mask everything that isn't background\n",
    "\n",
    "    # Step 2: Restrict search to central 60% of the image (to avoid edges)\n",
    "    height, width = gray_image.shape\n",
    "    x_start = int(width * 0.2)\n",
    "    x_end = int(width * 0.8)\n",
    "    y_start = int(height * 0.2)\n",
    "    y_end = int(height * 0.8)\n",
    "\n",
    "    # Step 3: Initialize variables for tracking the darkest patch\n",
    "    min_brightness = 255\n",
    "    macula_center = (0, 0)\n",
    "\n",
    "    # Step 4: Sliding window search (excluding the background)\n",
    "    for y in range(y_start, y_end - 30, 10):  \n",
    "        for x in range(x_start, x_end - 30, 10):  \n",
    "            patch = gray_image[y:y+30, x:x+30]\n",
    "            patch_mask = mask[y:y+30, x:x+30]  # Apply the mask to remove background\n",
    "\n",
    "            # Ignore patches that have a lot of background\n",
    "            if np.mean(patch_mask) < 200:  # If mask is mostly black, skip\n",
    "                continue  \n",
    "\n",
    "            mean_brightness = np.mean(patch)\n",
    "\n",
    "            if mean_brightness < min_brightness:\n",
    "                min_brightness = mean_brightness\n",
    "                macula_center = (x + 15, y + 15)  # Center of 30Ã—30 patch\n",
    "\n",
    "    return {'x': macula_center[0], 'y': macula_center[1]}\n",
    "\n",
    "# ---------------------------\n",
    "# **Notch Detection** (Identifying a Square, Triangle, or Circle at the Border)\n",
    "# ---------------------------\n",
    "def detect_notch(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Crop the edges of the image where the notch is expected\n",
    "    height, width = gray_image.shape\n",
    "    edge_width = int(width * 0.05)  # Adjust as needed\n",
    "\n",
    "    # Left edge\n",
    "    left_edge = gray_image[:, :edge_width]\n",
    "    # Right edge\n",
    "    right_edge = gray_image[:, -edge_width:]\n",
    "\n",
    "    # Combine edges\n",
    "    edges = [left_edge, right_edge]\n",
    "\n",
    "    for edge in edges:\n",
    "        _, thresh = cv2.threshold(edge, 35, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area > 100:  # Adjust threshold based on expected notch size\n",
    "                # Approximate contour to a polygon\n",
    "                approx = cv2.approxPolyDP(cnt, 0.04 * cv2.arcLength(cnt, True), True)\n",
    "                if len(approx) == 3 or len(approx) == 4 or len(approx) > 8:\n",
    "                    return True  # Notch detected\n",
    "    return False  # No notch detected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in labels_df.iterrows():\n",
    "    image_name = row['image']\n",
    "    print(f\"Processing image {image_name}\")\n",
    "    image = load_image(image_name)\n",
    "\n",
    "    if image is not None:\n",
    "        try:\n",
    "            inverted = is_inverted(image)\n",
    "            labels_df.at[index, 'inverted'] = inverted\n",
    "            print(f\"Image {image_name} inverted: {inverted}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_name}: {e}\")\n",
    "            labels_df.at[index, 'inverted'] = None\n",
    "    else:\n",
    "        print(f\"Skipping image {image_name} due to loading error.\")\n",
    "        labels_df.at[index, 'inverted'] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample image\n",
    "sample_image = load_image(labels_df.iloc[2]['image'])\n",
    "cv_image = cv2.cvtColor(np.array(sample_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Detect features\n",
    "optic_nerve_pos = detect_optic_nerve(cv_image)\n",
    "macula_pos = detect_macula(cv_image)\n",
    "notch_present = detect_notch(cv_image)\n",
    "\n",
    "print(f\"Optic Nerve Position: {optic_nerve_pos}\")\n",
    "print(f\"Macula Position: {macula_pos}\")\n",
    "print(f\"Notch Present: {notch_present}\")\n",
    "\n",
    "# Display the image with detected points\n",
    "if optic_nerve_pos and macula_pos:\n",
    "    cv2.circle(cv_image, (optic_nerve_pos['x'], optic_nerve_pos['y']), 50, (0, 255, 0), -1)\n",
    "    cv2.circle(cv_image, (macula_pos['x'], macula_pos['y']), 50, (0, 0, 255), -1)\n",
    "\n",
    "     # Create a resizable window\n",
    "    cv2.namedWindow('Detected Features', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Detected Features', 800, 600)  # Set to desired dimensions\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    cv2.imshow('Detected Features', cv_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
