{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      image  level  inverted\n",
      "0   10_left      0     False\n",
      "1  10_right      0     False\n",
      "2   13_left      0     False\n",
      "3  13_right      0     False\n",
      "4   15_left      1     False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "labels_df = pd.read_csv('G:\\\\Graduation Project\\\\trainLabels.csv')\n",
    "labels_df['inverted'] = False\n",
    "print(labels_df.head())\n",
    "\n",
    "image_dir = 'G:\\\\Graduation Project\\\\Images\\\\train\\\\' \n",
    "\n",
    "def load_image(image_name):\n",
    "    file_extension = '.jpeg'\n",
    "    filename = f\"{image_name}{file_extension}\"\n",
    "    file_path = os.path.join(image_dir, filename)\n",
    "    return Image.open(file_path)\n",
    "\n",
    "for i in range (1):\n",
    "    sample_image = load_image(labels_df.iloc[i]['image'])\n",
    "    sample_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inversion checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_inverted(image):\n",
    "    try:\n",
    "        # Ensure the image is fully loaded\n",
    "        image.load()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image data: {e}\")\n",
    "        raise e  # Re-raise the exception\n",
    "\n",
    "    try:\n",
    "        # Ensure image is in RGB mode\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        # Convert PIL image to OpenCV format\n",
    "        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting image to OpenCV format: {e}\")\n",
    "        raise e  # Re-raise the exception\n",
    "\n",
    "    try:\n",
    "        # Detect features\n",
    "        optic_nerve_pos = detect_optic_nerve(cv_image)\n",
    "        macula_pos = detect_macula(cv_image)\n",
    "        notch_present = detect_notch(cv_image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature detection: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Ensure features were detected\n",
    "    if optic_nerve_pos is None or macula_pos is None:\n",
    "        print(\"Could not detect optic nerve or macula.\")\n",
    "        return False  # Or handle as per your needs\n",
    "\n",
    "    # Calculate optic nerve midline (y-coordinate)\n",
    "    optic_nerve_midline_y = optic_nerve_pos['y']\n",
    "\n",
    "    # Compare macula position to optic nerve midline\n",
    "    macula_higher = macula_pos['y'] < optic_nerve_midline_y\n",
    "\n",
    "    # Determine inversion based on criteria\n",
    "    if (macula_higher) or (not notch_present):\n",
    "        # Image is inverted\n",
    "        return True\n",
    "    else:\n",
    "        # Image is not inverted\n",
    "        return False\n",
    "\n",
    "def correct_inversion(image):\n",
    "    # Flip the image vertically\n",
    "    corrected_image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "    return corrected_image\n",
    "\n",
    "\n",
    "    #<<functions for feature detection>>\n",
    "\n",
    "    # Optic never detection\n",
    "def detect_optic_nerve(image):\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "\n",
    "    # Apply Gaussian Blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "    # Apply thresholding to segment bright areas\n",
    "    _, thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Assume the largest bright area is the optic nerve\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        # Calculate the center of the optic nerve\n",
    "        M = cv2.moments(largest_contour)\n",
    "        if M['m00'] != 0:\n",
    "            cx = int(M['m10'] / M['m00'])  # x-coordinate\n",
    "            cy = int(M['m01'] / M['m00'])  # y-coordinate\n",
    "            return {'x': cx, 'y': cy}\n",
    "    return None\n",
    "\n",
    "\n",
    "#Macula detection\n",
    "\n",
    "def detect_macula(image):\n",
    "    # Convert to grayscale if necessary\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "\n",
    "    # Image dimensions\n",
    "    height, width = gray_image.shape\n",
    "\n",
    "    # Define the central region (e.g., central 50% of the image)\n",
    "    x_start = int(width * 0.2)\n",
    "    x_end = int(width * 0.75)\n",
    "    y_start = int(height * 0.2)\n",
    "    y_end = int(height * 0.75)\n",
    "\n",
    "    # Crop the central region\n",
    "    central_region = gray_image[y_start:y_end, x_start:x_end]\n",
    "\n",
    "    #Gaussian Blur \n",
    "    blurred = cv2.GaussianBlur( central_region, (9, 9), 0,cv2.BORDER_DEFAULT)\n",
    "\n",
    "    #Thresholding\n",
    "    res,thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Filter\n",
    "\n",
    "    # _, otsu_thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) Otsu's Thresholding could be used for better results\n",
    "\n",
    "    #Contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        print(\"No dark regions detected in the central area.\")\n",
    "        return {'x': 0, 'y': 0}\n",
    "    \n",
    "    # Assume the largest dark contour corresponds to the macula\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Calculate the centroid of the largest contour\n",
    "    M = cv2.moments(largest_contour)\n",
    "    if M['m00'] == 0:\n",
    "        print(\"Zero division error while calculating centroid.\")\n",
    "        return {'x': 0, 'y': 0}\n",
    "\n",
    "    cX = int(M['m10'] / M['m00']) + x_start\n",
    "    cY = int(M['m01'] / M['m00']) + y_start\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create a resizable window\n",
    "    cv2.namedWindow('Grayscale Image', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Grayscale Image', 800, 600)  # Set to desired dimensions\n",
    "    cv2.namedWindow('Blurred Image', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Blurred Image', 800, 600)  \n",
    "    cv2.namedWindow('Thresholded Central Region', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Thresholded Central Region', 800, 600)  \n",
    "    cv2.namedWindow('Detected Macula', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Detected Macula', 800, 600)  \n",
    "    cv2.namedWindow('Central Region', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Central Region', 800, 600)\n",
    "    cv2.namedWindow('res', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Central Region', 800, 600) \n",
    "    cv2.circle(image, (cX, cY), 30, (0, 0, 255), -1)  # Red circle for macula\n",
    "   \n",
    "\n",
    "    cv2.imshow('Grayscale Image', gray_image)\n",
    "    cv2.imshow('Blurred Image', blurred)\n",
    "    cv2.imshow('Thresholded Central Region', thresh)\n",
    "    cv2.imshow('res', res)\n",
    "    cv2.imshow('Detected Macula', image)\n",
    "    cv2.imshow('Central Region', central_region)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    return {'x': cX, 'y': cY}\n",
    "\n",
    "#Notch detection\n",
    "def detect_notch(image):\n",
    "    # Convert to grayscale if necessary\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "\n",
    "    # Crop the edges of the image where the notch is expected\n",
    "    height, width = gray_image.shape\n",
    "    edge_width = int(width * 0.05)  # Adjust as needed\n",
    "\n",
    "    # Left edge\n",
    "    left_edge = gray_image[:, :edge_width]\n",
    "    # Right edge\n",
    "    right_edge = gray_image[:, -edge_width:]\n",
    "\n",
    "    # Combine edges\n",
    "    edges = [left_edge, right_edge]\n",
    "\n",
    "    # Look for contours in the edge regions\n",
    "    for edge in edges:\n",
    "        # Apply thresholding\n",
    "        _, thresh = cv2.threshold(edge, 35, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Analyze contours\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area > 100:  # Adjust threshold based on expected notch size\n",
    "                # Approximate contour to a polygon\n",
    "                approx = cv2.approxPolyDP(cnt, 0.04 * cv2.arcLength(cnt, True), True)\n",
    "                # Check for square, triangle, or circle\n",
    "                if len(approx) == 3 or len(approx) == 4 or len(approx) > 8:\n",
    "                    return True  # Notch detected\n",
    "    return False  # No notch detected \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 10_left\n",
      "No dark regions detected in the central area.\n",
      "Image 10_left inverted: True\n",
      "Processing image 10_right\n",
      "No dark regions detected in the central area.\n",
      "Could not detect optic nerve or macula.\n",
      "Image 10_right inverted: False\n",
      "Processing image 13_left\n",
      "No dark regions detected in the central area.\n",
      "Image 13_left inverted: True\n",
      "Processing image 13_right\n",
      "No dark regions detected in the central area.\n",
      "Image 13_right inverted: True\n",
      "Processing image 15_left\n",
      "No dark regions detected in the central area.\n",
      "Could not detect optic nerve or macula.\n",
      "Image 15_left inverted: False\n",
      "Processing image 15_right\n",
      "No dark regions detected in the central area.\n",
      "Image 15_right inverted: True\n",
      "Processing image 16_left\n",
      "No dark regions detected in the central area.\n",
      "Image 16_left inverted: True\n",
      "Processing image 16_right\n",
      "No dark regions detected in the central area.\n",
      "Image 16_right inverted: True\n",
      "Processing image 17_left\n",
      "No dark regions detected in the central area.\n",
      "Image 17_left inverted: True\n",
      "Processing image 17_right\n",
      "No dark regions detected in the central area.\n",
      "Image 17_right inverted: True\n",
      "Processing image 19_left\n",
      "No dark regions detected in the central area.\n",
      "Image 19_left inverted: True\n",
      "Processing image 19_right\n",
      "No dark regions detected in the central area.\n",
      "Image 19_right inverted: True\n",
      "Processing image 20_left\n",
      "No dark regions detected in the central area.\n",
      "Image 20_left inverted: True\n",
      "Processing image 20_right\n",
      "No dark regions detected in the central area.\n",
      "Image 20_right inverted: True\n",
      "Processing image 21_left\n",
      "No dark regions detected in the central area.\n",
      "Image 21_left inverted: True\n",
      "Processing image 21_right\n",
      "No dark regions detected in the central area.\n",
      "Image 21_right inverted: True\n",
      "Processing image 22_left\n",
      "No dark regions detected in the central area.\n",
      "Image 22_left inverted: True\n",
      "Processing image 22_right\n",
      "No dark regions detected in the central area.\n",
      "Could not detect optic nerve or macula.\n",
      "Image 22_right inverted: False\n",
      "Processing image 23_left\n",
      "No dark regions detected in the central area.\n",
      "Could not detect optic nerve or macula.\n",
      "Image 23_left inverted: False\n",
      "Processing image 23_right\n",
      "No dark regions detected in the central area.\n",
      "Could not detect optic nerve or macula.\n",
      "Image 23_right inverted: False\n",
      "Processing image 25_left\n",
      "No dark regions detected in the central area.\n",
      "Could not detect optic nerve or macula.\n",
      "Image 25_left inverted: False\n",
      "Processing image 25_right\n",
      "No dark regions detected in the central area.\n",
      "Image 25_right inverted: True\n",
      "Processing image 30_left\n",
      "No dark regions detected in the central area.\n",
      "Image 30_left inverted: True\n",
      "Processing image 30_right\n",
      "No dark regions detected in the central area.\n",
      "Could not detect optic nerve or macula.\n",
      "Image 30_right inverted: False\n",
      "Processing image 31_left\n",
      "No dark regions detected in the central area.\n",
      "Image 31_left inverted: True\n",
      "Processing image 31_right\n",
      "No dark regions detected in the central area.\n",
      "Image 31_right inverted: True\n",
      "Processing image 33_left\n",
      "No dark regions detected in the central area.\n",
      "Image 33_left inverted: True\n",
      "Processing image 33_right\n",
      "No dark regions detected in the central area.\n",
      "Could not detect optic nerve or macula.\n",
      "Image 33_right inverted: False\n",
      "Processing image 36_left\n",
      "No dark regions detected in the central area.\n",
      "Image 36_left inverted: True\n",
      "Processing image 36_right\n",
      "No dark regions detected in the central area.\n",
      "Could not detect optic nerve or macula.\n",
      "Image 36_right inverted: False\n",
      "Processing image 40_left\n",
      "No dark regions detected in the central area.\n",
      "Image 40_left inverted: True\n",
      "Processing image 40_right\n",
      "No dark regions detected in the central area.\n",
      "Image 40_right inverted: True\n",
      "Processing image 41_left\n",
      "No dark regions detected in the central area.\n",
      "Image 41_left inverted: True\n",
      "Processing image 41_right\n",
      "No dark regions detected in the central area.\n",
      "Image 41_right inverted: True\n",
      "Processing image 42_left\n",
      "No dark regions detected in the central area.\n",
      "Image 42_left inverted: True\n",
      "Processing image 42_right\n",
      "No dark regions detected in the central area.\n",
      "Image 42_right inverted: True\n",
      "Processing image 46_left\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m image_name \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(image_name)\u001b[0m\n\u001b[0;32m     18\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfile_extension\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, filename)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Python\\lib\\site-packages\\PIL\\Image.py:3466\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3464\u001b[0m filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[1;32m-> 3466\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrealpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m   3469\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Python\\lib\\ntpath.py:660\u001b[0m, in \u001b[0;36mrealpath\u001b[1;34m(path, strict)\u001b[0m\n\u001b[0;32m    658\u001b[0m     path \u001b[38;5;241m=\u001b[39m join(cwd, path)\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 660\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43m_getfinalpathname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    661\u001b[0m     initial_winerror \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, row in labels_df.iterrows():\n",
    "    image_name = row['image']\n",
    "    print(f\"Processing image {image_name}\")\n",
    "    image = load_image(image_name)\n",
    "\n",
    "    if image is not None:\n",
    "        try:\n",
    "            inverted = is_inverted(image)\n",
    "            labels_df.at[index, 'inverted'] = inverted\n",
    "            print(f\"Image {image_name} inverted: {inverted}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_name}: {e}\")\n",
    "            labels_df.at[index, 'inverted'] = None\n",
    "    else:\n",
    "        print(f\"Skipping image {image_name} due to loading error.\")\n",
    "        labels_df.at[index, 'inverted'] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optic Nerve Position: {'x': 1462, 'y': 1321}\n",
      "Macula Position: {'x': 1846, 'y': 1230}\n",
      "Notch Present: False\n"
     ]
    }
   ],
   "source": [
    "# Test on a sample image\n",
    "sample_image = load_image(labels_df.iloc[8]['image'])\n",
    "cv_image = cv2.cvtColor(np.array(sample_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Detect features\n",
    "optic_nerve_pos = detect_optic_nerve(cv_image)\n",
    "macula_pos = detect_macula(cv_image)\n",
    "notch_present = detect_notch(cv_image)\n",
    "\n",
    "print(f\"Optic Nerve Position: {optic_nerve_pos}\")\n",
    "print(f\"Macula Position: {macula_pos}\")\n",
    "print(f\"Notch Present: {notch_present}\")\n",
    "\n",
    "# Display the image with detected points\n",
    "if optic_nerve_pos and macula_pos:\n",
    "    cv2.circle(cv_image, (optic_nerve_pos['x'], optic_nerve_pos['y']), 50, (0, 255, 0), -1)\n",
    "    cv2.circle(cv_image, (macula_pos['x'], macula_pos['y']), 50, (0, 0, 255), -1)\n",
    "\n",
    "     # Create a resizable window\n",
    "    cv2.namedWindow('Detected Features', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Detected Features', 800, 600)  # Set to desired dimensions\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    cv2.imshow('Detected Features', cv_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
